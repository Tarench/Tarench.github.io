(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{343:function(e,t,r){"use strict";r.r(t);var n=r(1),o=Object(n.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("p",[e._v("整理一下Transformer的一些东西，之前一直用，但是不太懂，只会用\n基于 "),t("a",{attrs:{href:"http://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[e._v("The Illustrated Transformer"),t("OutboundLink")],1),e._v(" 这篇博客做的笔记\n听说 Attention is All You Need 论文讲的很好，有空看了会整理进来\n")]),e._v(" "),t("p",[e._v("https://www.youtube.com/watch?v=UNmqTiOnRfg")]),e._v(" "),t("p",[e._v("https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/")]),e._v(" "),t("p",[e._v("http://jalammar.github.io/illustrated-transformer/")]),e._v(" "),t("h1",{attrs:{id:"_1-attention"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-attention"}},[e._v("#")]),e._v(" 1.Attention")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("A sequence-to-sequence model is a model that "),t("strong",[e._v("takes a sequence of items")]),e._v(" (words, letters, features of an images…etc) and "),t("strong",[e._v("outputs another sequence of items")])])]),e._v(" "),t("li",[t("p",[e._v("Under the hood, the model is composed of an encoder and a decoder.")])])]),e._v(" "),t("h2",{attrs:{id:"_1-1-encoder-decoder"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-encoder-decoder"}},[e._v("#")]),e._v(" 1.1 Encoder-Decoder")]),e._v(" "),t("blockquote",[t("ul",[t("li",[e._v("The encoder processes each item in the input sequence, it compiles the information it captures into a "),t("strong",[e._v("vector")]),e._v(" (called the "),t("strong",[e._v("context")]),e._v(").")]),e._v(" "),t("li",[e._v("After processing the entire input sequence, the "),t("strong",[e._v("encoder")]),e._v(" sends the "),t("strong",[e._v("contex")]),e._v("t over to the "),t("strong",[e._v("decoder")]),e._v(", which begins producing the output sequence item by item")])])]),e._v(" "),t("ul",[t("li",[t("p",[e._v("The encoder and decoder tend to both be r"),t("strong",[e._v("ecurrent neural networks")]),e._v(" ("),t("a",{attrs:{href:"https://www.youtube.com/watch?v=UNmqTiOnRfg",target:"_blank",rel:"noopener noreferrer"}},[e._v("RNNs"),t("OutboundLink")],1),e._v(")")])]),e._v(" "),t("li",[t("p",[e._v("The size of the "),t("strong",[e._v("context")]),e._v(" vector is basically the number of hidden units in the "),t("strong",[e._v("encoder")]),e._v(" RNN.")])])]),e._v(" "),t("blockquote",[t("p",[e._v("A RNN takes two inputs at each time step")]),e._v(" "),t("ol",[t("li",[e._v("an input (in the case of the encoder, one word from the input sentence)")]),e._v(" "),t("li",[e._v("and a hidden state")])])]),e._v(" "),t("ul",[t("li",[e._v("Transform a "),t("strong",[e._v("word")]),e._v(" into a "),t("strong",[e._v("vector")]),e._v(" by "),t("strong",[e._v('"word embedding"')]),e._v(" algorithma")])]),e._v(" "),t("h3",{attrs:{id:"recurrent-neural-network"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#recurrent-neural-network"}},[e._v("#")]),e._v(" Recurrent Neural Network")])])}),[],!1,null,null,null);t.default=o.exports}}]);